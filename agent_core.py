import torch
import os
from transformers import AutoTokenizer, AutoModelForCausalLM

# =========================================================================
# === 1. CONFIGURA√á√ïES GLOBAIS: CodeGemma 2B e CPU (SOLU√á√ÉO FINAL) ===
# =========================================================================

# Vari√°vel GLOBAL: Modelo CodeGemma 2B Instruct (Livre de licen√ßa e ultraleve)
MODEL_NAME = "codegemma-2b-it" 

# For√ßar a execu√ß√£o na CPU para garantir estabilidade (ignora a GPU incompat√≠vel)
device = "cpu"
device_map_setting = "cpu" 
print(f"Dispositivo de processamento definido: {device}")


# =========================================================================
# === 2. FUN√á√ÉO DE CARREGAMENTO DO MODELO ===
# =========================================================================

def load_code_llama(): # Mantemos o nome da fun√ß√£o por conveni√™ncia, embora carregue CodeGemma
    print(f"Tentando carregar o modelo: {MODEL_NAME}")
    
    try:
        # Carregar o Tokenizer (N√£o requer 'use_auth_token' para CodeGemma)
        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME) 
        
        # Carregar o Modelo CodeGemma 2B 
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_NAME, 
            torch_dtype=torch.float16, # Mant√©m a economia de mem√≥ria
            device_map=device_map_setting, # Garante que vai para a CPU
        )
        
        tokenizer.pad_token = tokenizer.eos_token 
        
        print(f"\n‚úÖ Modelo {MODEL_NAME} e Tokenizer carregados com sucesso!")
        print(f"Configura√ß√£o do dispositivo: {model.device}") 
        
        return tokenizer, model
        
    except Exception as e:
        print("\n‚ùå ERRO ao carregar o modelo:")
        print(f"Detalhes do erro: {e}")
        return None, None


# =========================================================================
# === 3. M√ìDULO: Gera√ß√£o de Cen√°rios de Teste (Fase 2) ===
# =========================================================================

def generate_test_scenarios(tokenizer, model, user_story):
    
    system_prompt = (
        "Voc√™ √© um Engenheiro de Qualidade de Software (QA) s√™nior e met√≥dico. "
        "Sua tarefa √© analisar a User Story fornecida e gerar uma lista detalhada de "
        "cen√°rios de teste. Garanta que a lista cubra os seguintes tipos de teste: "
        "1. Happy Path (Sucesso); 2. Boundary Cases (Limites); 3. Negative Cases (Erros/Seguran√ßa). "
        "Apresente a sa√≠da como uma lista numerada e clara."
    )
    
    # Formato de Prompt espec√≠fico para o CodeGemma Instruct
    prompt = f"""<start_of_turn>user
    {system_prompt}
    
    User Story: "{user_story}"
    
    Gere os cen√°rios de teste:<end_of_turn>
    <start_of_turn>model
    """

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    # Configura√ß√µes de gera√ß√£o ajustadas para CPUs
    output = model.generate(
        **inputs, 
        max_new_tokens=400,       
        do_sample=True,
        temperature=0.7,          
        top_p=0.9,                
        pad_token_id=tokenizer.eos_token_id
    )

    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)
    
    # O CodeGemma tem um formato de resposta espec√≠fico, precisamos limpar apenas a resposta
    response_start_tag = "<start_of_turn>model"
    response_start = generated_text.find(response_start_tag)
    
    if response_start != -1:
        # Retorna o texto ap√≥s a tag de resposta
        return generated_text[response_start + len(response_start_tag):].strip()
    
    return "Erro ao gerar cen√°rios."


# =========================================================================
# === 4. EXECU√á√ÉO PRINCIPAL ===
# =========================================================================

# Chamada da fun√ß√£o para carregar o modelo
tokenizer, model = load_code_llama()

if model:
    # Cen√°rio de teste complexo
    user_story_exemplo = (
        "Como usu√°rio, quero me cadastrar em um novo sistema. O campo 'Nome' deve ter "
        "no m√≠nimo 3 e no m√°ximo 50 caracteres. O campo 'Senha' deve ter entre 8 e 16 caracteres, "
        "incluindo pelo menos uma letra mai√∫scula e um n√∫mero. O campo 'Email' deve ser √∫nico e formatado corretamente."
    )
    
    print("\n--- Processando User Story... ---")
    
    # Chamada da nova fun√ß√£o!
    cenarios = generate_test_scenarios(tokenizer, model, user_story_exemplo)
    
    print("\n=================================")
    print("ü§ñ Cen√°rios Gerados pelo AI-QA Agent:")
    print("=================================")
    print(cenarios)
    print("---------------------------------")